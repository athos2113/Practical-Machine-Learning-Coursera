---
title: "Machine Learning Project"
author: "Junaid Khan"
date: "May 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.


##Libraries
```{r cars}

library(caret)
library(rpart)
library(rattle)

```

### Reading the Train and Test csv files 
```{r , cache=TRUE}

Training <- read.csv("pml-training.csv")
Testing <- read.csv("pml-testing.csv")

```

Looking at the Training Data strucutre
```{r}
str(Training)
```

The training data set is made of 19622 observations on 160 columns.
We can notice that many columns have NA values or blank values on
almost every observation. So we will remove them, because they will
not produce any information. The first seven columns give information
about the people who did the test, and also timestamps. We will not take them in our model.

###Here we get the indexes of the columns having at least 90% of NA or blank values on the training dataset
```{r}
Training <- read.csv("pml-training.csv")
colToRemove <- which(colSums(is.na(Training) | Training =="")> 0.9*dim(Training)[1])
TrainingClean <- Training[,-colToRemove]
TrainingClean <- TrainingClean[,-c(1:7)]
dim(TrainingClean)
```

###Do the same for Testing Data
```{r}
Testing <- read.csv("pml-testing.csv")
colToRemove <- which(colSums(is.na(Testing) | Testing =="") > 0.9*dim(Testing)[1])
TestingClean <- Testing[,-colToRemove]
TestingClean <- TestingClean[,-1]
```

```{r}
# Here we create a partition of the training data set 
set.seed(12345)
inTrain1 <- createDataPartition(TrainingClean$classe, p=0.75, list=FALSE)
Train1 <- TrainingClean[inTrain1,]
Test1 <- TrainingClean[-inTrain1,]
dim(Train1)
dim(Test1)
```

In the following sections, we will test 3 different models : Classification Tree, Random Forest
and Gradient Boosting

In order to avoid the problem of overfitting, we will use cross-validation. We will use
the **k-fold technique, with 5 folds.**

###Train with Classification Tree

```{r}
# define training control
train_control<- trainControl(method="cv", number=10)
```
```{r , cache=TRUE}  
  # Building the classifier 
model1 <- train(classe ~ . ,data = Train1, trControl = train_control, method="rpart")


 # Plot Classification Tree
fancyRpartPlot(model1$finalModel)

  # Predicting With Classification Tree
trainpred <- predict(model1, newdata = Test1)

  #Building Confusion Matrix

confusion_CT <- confusionMatrix(Test1$classe,trainpred)
confusion_CT$table
confusion_CT$overall[1]
```

We can notice that the accuracy is **very low ( around 55 %)**. Thus this model
wont be appropriate to make prediction.


##Train with Random Forest 
```{r ,cache=TRUE}
  #Building the classifier 
model2 <- train(classe~., data=Train1, method="rf", trControl=train_control, verbose=FALSE)
```

```{r}
  # Plotting 
plot(model2,main="Accuracy of Random forest model by number of predictors")

  # Predicting with Random forest
trainpred2 <- predict(model2, newdata = Test1)

  #Building the Confusion Matrix
confusion_RF <- confusionMatrix(Test1$classe,trainpred2)
confusion_RF$table
confusion_RF$overall[1]
  
  #Model Error Plot
plot(model2$finalModel,main="Model error of Random forest model by number of trees")

  #Compute the variable importance 
MostImpVars <- varImp(model2)
MostImpVars
```

With random forest, we reach an **accuracy of 99.3%** using cross-validation
with 5 steps. This is very good. But let's see what we can expect with Gradient boosting.
    
We can also notice that the optimal number of predictors, i.e.
the number of predictors giving the highest accuracy, is 27.
There is no significal increase of the accuracy with 2 predictors and 27,
but the slope decreases more with more than 27 predictors
(even if the accuracy is still very good).
The fact that not all the accuracy is worse with all the available
predictors lets us suggest that there may be some dependencies between them.

##Train with Gradient Boosting
```{r,cache=TRUE}  
  # Building the classifier
model3 <- train(classe~., data=Train1, method="gbm", trControl=train_control, verbose=FALSE)
print(model3)
```

```{r}
  #plotting the classifier
plot(model3)

  #Predicting with Gradient Boosting
trainpred3 <- predict(model3, newdata = Test1)
confusion_GB <- confusionMatrix(Test1$classe,trainpred3)
confusion_GB$table

confusion_GB$overall[1]
```
**Precision with 5 folds is 96%.**

##Conclusion :
Thus we observe that **Random Forest is the best model with 99% accuracy.** 
We will use this model to predict the values of the classe for the test set.
```{r}
testpred <- predict(model2, newdata=TestingClean)
testpred
```